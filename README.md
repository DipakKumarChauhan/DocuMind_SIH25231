# DocuMind - Multimodal RAG System

## Problem Statement

A semantic search and RAG system that indexes multiple data modalities (PDFs, DOCX, images, audio) into a unified semantic space, enabling users to query across all formats and receive grounded, citation-based answers from an LLM.

## Features (Text-Only Prototype)

- ğŸ“„ Multi-format document ingestion (PDF, DOCX, TXT)
- ğŸ” Semantic search with sentence-transformers embeddings
- ğŸ¯ Citation-based answers with source traceability
- ğŸ’¾ Persistent vector storage with ChromaDB
- ğŸš€ Fast Streamlit UI for demo
- ğŸ“¦ Modular, production-ready architecture

## Architecture

```
DocuMind/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”‚   â”œâ”€â”€ logger.py          # Logging setup
â”‚   â”‚   â””â”€â”€ exceptions.py      # Custom exceptions
â”‚   â”œâ”€â”€ document_processing/
â”‚   â”‚   â”œâ”€â”€ extractors.py      # Text extraction from PDF/DOCX
â”‚   â”‚   â”œâ”€â”€ chunker.py         # Smart text chunking with overlap
â”‚   â”‚   â””â”€â”€ preprocessor.py    # Text cleaning & normalization
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”œâ”€â”€ generator.py       # Embedding generation
â”‚   â”‚   â””â”€â”€ cache.py           # Embedding cache management
â”‚   â”œâ”€â”€ vector_store/
â”‚   â”‚   â”œâ”€â”€ client.py          # ChromaDB client wrapper
â”‚   â”‚   â””â”€â”€ indexer.py         # Document indexing logic
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ retriever.py       # Similarity search & ranking
â”‚   â”‚   â””â”€â”€ reranker.py        # Optional reranking logic
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”œâ”€â”€ llm_client.py      # LLM API client (OpenAI/local)
â”‚   â”‚   â”œâ”€â”€ prompt_templates.py # RAG prompt templates
â”‚   â”‚   â””â”€â”€ citation_parser.py # Parse & validate citations
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ models.py          # Pydantic models
â”‚       â””â”€â”€ service.py         # Main RAG service orchestration
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py       # Streamlit frontend
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extractors.py
â”‚   â”œâ”€â”€ test_chunker.py
â”‚   â””â”€â”€ test_retrieval.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/               # User uploaded documents
â”‚   â”œâ”€â”€ vectordb/              # ChromaDB persistent storage
â”‚   â””â”€â”€ cache/                 # Embedding cache
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## Installation

```bash
# Clone the repository
cd /home/dipak/DocuMind_cursor

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download NLTK data
python -c "import nltk; nltk.download('punkt')"

# Setup environment
cp .env.example .env
# Edit .env with your OpenAI API key or local LLM endpoint
```

## Usage

```bash
# Run Streamlit app
streamlit run ui/streamlit_app.py

# Or use programmatically
python -c "from src.api.service import RAGService; service = RAGService()"
```

## Key Design Decisions

### Chunking Strategy

- **Chunk size**: 300 tokens (~200-400 words)
- **Overlap**: 50 tokens to preserve context
- **Method**: Sentence-aware splitting using NLTK
- **Why**: Balance between precision (small chunks) and context (overlap)

### Embedding Model

- **Model**: `sentence-transformers/all-MiniLM-L6-v2`
- **Dimension**: 384
- **Why**: Fast, lightweight, good semantic understanding

### Vector Store

- **Primary**: ChromaDB (local persistence)
- **Alternative**: FAISS (in-memory, faster)
- **Why**: ChromaDB offers metadata filtering + persistence out of the box

### Retrieval

- **Top-K**: 3-5 chunks
- **Similarity**: Cosine similarity
- **Metadata**: file_id, page, chunk_id, char offsets for precise citation

### LLM Integration

- **Default**: OpenAI GPT-3.5/4
- **Fallback**: Local models via Ollama
- **Prompt**: System prompt enforces citation format [1], [2]

## Roadmap

### Phase 1: Text-only RAG âœ…

- PDF, DOCX, TXT extraction
- Semantic chunking & indexing
- Citation-based answers

### Phase 2: Image Support (Future)

- OCR with Tesseract/EasyOCR
- CLIP embeddings for images
- Unified text+image retrieval

### Phase 3: Audio Support (Future)

- Whisper STT for transcription
- Time-stamped chunk indexing
- Audio playback from citations

## Success Metrics

- âœ… End-to-end demo: Upload â†’ Query â†’ Cited Answer
- âœ… Clickable citations linking to source
- âœ… Retrieval precision validated manually
- âœ… Response time < 3s for typical query

## License

MIT
